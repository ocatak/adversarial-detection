{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Main_All_Digit_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zPEpf8l2gBa",
        "collapsed": true
      },
      "source": [
        "!pip install foolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCrlI0Wv2hpK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krHvcTUu2lmS"
      },
      "source": [
        "!git clone https://github.com/knamdar/data.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZVzd3GH2uyN"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import torch.optim as optim\n",
        "import foolbox as fb\n",
        "from foolbox import PyTorchModel, accuracy, samples\n",
        "from foolbox.attacks import LinfPGD,LinfBasicIterativeAttack,LinfFastGradientAttack,L2CarliniWagnerAttack,LinfDeepFoolAttack,L2DeepFoolAttack,L2PGD\n",
        "from time import gmtime, strftime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import multiprocessing as mp\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXsih9yG25GM"
      },
      "source": [
        "if not os.path.exists('/content/gdrive/MyDrive/checkpointDigitMNIST'):\n",
        "    os.makedirs('/content/gdrive/MyDrive/checkpointDigitMNIST')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_qPWvrl4jvE"
      },
      "source": [
        "class LeNet_dropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet_dropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
        "        self.fc1 = nn.Linear(2880, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.drop_layer = nn.Dropout(p=0.5)\n",
        "\n",
        "    def last_hidden_layer_output(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.drop_layer(F.max_pool2d(F.relu(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 2880)\n",
        "        x = self.drop_layer(F.relu(self.fc1(x)))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.last_hidden_layer_output(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MLP,self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "\n",
        "      nn.Linear(128, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, 1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(1024, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bvJ9_DB2oQf"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data = datasets.MNIST(root='data', train=True, download=False, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='data', train=False, download=False, transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "\n",
        "learning_rate = 0.001\n",
        "epoch = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srHI7Q133bg0"
      },
      "source": [
        "model_dropout = LeNet_dropout()\n",
        "model_dropout = model_dropout.to(device)\n",
        "\n",
        "optimizer_dropout = optim.Adam(model_dropout.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zOncwvO3qbN"
      },
      "source": [
        "\n",
        "def train(model, opt, epoch):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    lr = opt.param_groups[0]['lr']\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(F.log_softmax(output, dim=1), target)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)] lr: {}\\tLoss: {:.6f}'\n",
        "                  .format(epoch, batch_idx * len(data),\n",
        "                          len(train_loader.dataset),\n",
        "                          100. * batch_idx / len(train_loader),\n",
        "                          lr, loss.data))\n",
        "\n",
        "def test(model):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(F.log_softmax(output, dim=1), target, size_average=False).data# sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def mcdropout_test(model):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    enable_dropout(model)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    T = 50\n",
        "\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output_list = []\n",
        "\n",
        "        for i in range(T):\n",
        "            output_list.append(torch.unsqueeze(model(data), 0))\n",
        "\n",
        "        output_mean = torch.cat(output_list, 0).mean(0)\n",
        "        test_loss += F.nll_loss(F.log_softmax(output_mean, dim=1), target, size_average=False).data  # sum up batch loss\n",
        "        pred = output_mean.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nMC Dropout Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "def enable_dropout(model):\n",
        "    \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__.startswith('Dropout'):\n",
        "            m.train()\n",
        "\n",
        "def noise(x, eps, clip_min, clip_max):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    eta = torch.FloatTensor(*x.shape).normal_(mean=0,std=eps)\n",
        "    eta = eta.to(device)\n",
        "    adv_x = x + eta\n",
        "    if clip_min is not None and clip_max is not None:\n",
        "        adv_x = torch.clamp(adv_x, min=clip_min, max=clip_max)\n",
        "    return adv_x\n",
        "\n",
        "def predict_uncertainties(model, image, T=50):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    image = image.to(device)\n",
        "\n",
        "    #torch.manual_seed(2)\n",
        "\n",
        "    image = image.detach()\n",
        "    item_count = image.shape[0]\n",
        "\n",
        "    enable_dropout(model)\n",
        "    model.train()\n",
        "\n",
        "    dropout_predictions = torch.zeros([T, item_count, 10])\n",
        "\n",
        "    for t in range(T):\n",
        "\n",
        "        enable_dropout(model)\n",
        "        model.train()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "\n",
        "        output_prob = F.softmax(output, dim=1) #shape is 1x10 if item_count is 1 (only one image in input batch)\n",
        "        dropout_predictions[t] = output_prob\n",
        "\n",
        "    #dropout_predictions is of shape 50xitem_countx10\n",
        "\n",
        "    # print(\"dropout predictions shape\", dropout_predictions.shape)\n",
        "\n",
        "    mean = torch.mean(dropout_predictions, dim=0)\n",
        "\n",
        "    entropy = Categorical(probs=mean).entropy()\n",
        "\n",
        "    pred_mean = mean\n",
        "\n",
        "    aleatoric = torch.zeros([item_count,10,10])\n",
        "    epistemic = torch.zeros([item_count,10,10])\n",
        "\n",
        "    for t in range(T):\n",
        "\n",
        "        pred_t = dropout_predictions[t]\n",
        "\n",
        "        aleatoric += torch.diag_embed(pred_t, offset=0, dim1=-2, dim2=-1) - pred_t[:, :, None] @ pred_t[:, None, :]\n",
        "        epistemic += (pred_t - pred_mean)[:, :, None] @ (pred_t - pred_mean)[:, None, :]\n",
        "\n",
        "    aleatoric = aleatoric / T #both of them are of shape item_count x 10x10\n",
        "    epistemic = epistemic / T #both of them are of shape item_count x 10x10\n",
        "\n",
        "    # print(\"aleatoric.shape\", aleatoric.shape)\n",
        "    # print(\"aleatoric.diag.shape\", torch.diagonal(aleatoric, 0, dim1=-2, dim2=-1).shape)\n",
        "\n",
        "    aleatoric = torch.diagonal(aleatoric, 0, dim1=-2, dim2=-1)\n",
        "    epistemic = torch.diagonal(epistemic, 0, dim1=-2, dim2=-1)\n",
        "\n",
        "    aleatoric = torch.mean(aleatoric,1,True)\n",
        "    epistemic = torch.mean(epistemic, 1, True)\n",
        "\n",
        "    scibilic = epistemic / aleatoric\n",
        "\n",
        "    scibilic[torch.isnan(scibilic)] = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return aleatoric.transpose_(0, 1)[0], epistemic.transpose_(0, 1)[0], scibilic.transpose_(0, 1)[0], entropy\n",
        "\n",
        "def make_long(a,n):\n",
        "    return np.tile(a,n)\n",
        "\n",
        "def stable_softmax(x):\n",
        "    numerator = np.exp(x)\n",
        "    denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
        "    softmax = numerator / denominator\n",
        "    return softmax\n",
        "\n",
        "def get_mean(x):\n",
        "    return x / np.sum(x, axis=-1, keepdims=True)\n",
        "\n",
        "def calculate_distance(model, predictions, last_hidden_layer_outs):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total = []\n",
        "\n",
        "    for z in range(last_hidden_layer_outs.shape[0]):\n",
        "\n",
        "        hidden_tensor = torch.from_numpy(last_hidden_layer_outs[z])\n",
        "\n",
        "        hidden_tensor = hidden_tensor.to(device)\n",
        "\n",
        "        hidden_tensor = torch.unsqueeze(hidden_tensor, 0)\n",
        "\n",
        "        total.append(F.softmax(model(hidden_tensor) / 1, dim=1)[0][predictions[z].item()])\n",
        "\n",
        "    return torch.Tensor(total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv0gjB7S3upu"
      },
      "source": [
        "print(\"Press 0, 1, 2 , 3 , 4 , 5 :\\ntrain mode (0)\\ntest set uncertainty metrics for correct and wrong predictions (1)\\nPrepare Last Hidden Layer Outputs (2)\\nTrain MLP (3)\\nPrepare Data (4)\\nPlot Performances (5)\\n\")\n",
        "\n",
        "input_a = int(input())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg6zbegU3xIo"
      },
      "source": [
        "if input_a == 0:\n",
        "\n",
        "    print ('Train Lenet with dropout at all layer')\n",
        "    for epoch in range(1, epoch + 1):\n",
        "        train(model_dropout, optimizer_dropout, epoch)\n",
        "    print(\"Test Set results of dropout model\")\n",
        "    test(model_dropout)\n",
        "    print(\"MC Dropout Test Results of dropout model\")\n",
        "    mcdropout_test(model_dropout)\n",
        "\n",
        "    print('Save /content/gdrive/MyDrive/checkpointDigitMNIST/' + 'LeNet_dropout' + str(epoch) + '.pth.tar')\n",
        "    state = {'state_dict': model_dropout.state_dict()}\n",
        "    filename = '/content/gdrive/MyDrive/checkpointDigitMNIST/' + 'LeNet_dropout' + str(epoch) + '.pth.tar'\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcl4UhRw3zyg"
      },
      "source": [
        "if input_a == 1:\n",
        "\n",
        "    ckpt_dropout = torch.load('/content/gdrive/MyDrive/checkpointDigitMNIST/LeNet_dropout10.pth.tar')\n",
        "    model_dropout.load_state_dict(ckpt_dropout['state_dict'])\n",
        "    model_dropout.eval()\n",
        "    model_dropout = model_dropout.to(device)\n",
        "\n",
        "    print(\"Beginning time is : \")\n",
        "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "    first_time = datetime.datetime.now()\n",
        "\n",
        "    al_clean_tensor_corrects = torch.empty(0)\n",
        "    ep_clean_tensor_corrects = torch.empty(0)\n",
        "    sc_clean_tensor_corrects = torch.empty(0)\n",
        "    ent_clean_tensor_corrects = torch.empty(0)\n",
        "\n",
        "    al_clean_tensor_wrongs = torch.empty(0)\n",
        "    ep_clean_tensor_wrongs = torch.empty(0)\n",
        "    sc_clean_tensor_wrongs = torch.empty(0)\n",
        "    ent_clean_tensor_wrongs = torch.empty(0)\n",
        "\n",
        "    a = 0\n",
        "\n",
        "    for test_images, test_labels in test_loader:\n",
        "\n",
        "        test_images = test_images.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "\n",
        "        model_dropout.eval()\n",
        "        sample_image = test_images\n",
        "        sample_label = test_labels\n",
        "        outputs = model_dropout(test_images)\n",
        "        corrects = (outputs.max(dim=1)[1] == test_labels)\n",
        "        wrongs = (outputs.max(dim=1)[1] != test_labels)\n",
        "\n",
        "        sample_image_corrects = sample_image[corrects]\n",
        "        sample_label_corrects = sample_label[corrects]\n",
        "\n",
        "        sample_image_wrongs = sample_image[wrongs]\n",
        "        sample_label_wrongs = sample_label[wrongs]\n",
        "\n",
        "\n",
        "        al_clean_corrects, ep_clean_corrects, sc_clean_corrects, ent_clean_corrects = predict_uncertainties(model_dropout, sample_image_corrects, 50)\n",
        "\n",
        "        al_clean_tensor_corrects = torch.cat([al_clean_tensor_corrects, al_clean_corrects])\n",
        "        ep_clean_tensor_corrects = torch.cat([ep_clean_tensor_corrects, ep_clean_corrects])\n",
        "        sc_clean_tensor_corrects = torch.cat([sc_clean_tensor_corrects, sc_clean_corrects])\n",
        "        ent_clean_tensor_corrects = torch.cat([ent_clean_tensor_corrects, ent_clean_corrects])\n",
        "\n",
        "        if sample_image_wrongs.shape[0] != 0:\n",
        "\n",
        "            al_clean_wrongs, ep_clean_wrongs, sc_clean_wrongs, ent_clean_wrongs = predict_uncertainties(model_dropout, sample_image_wrongs, 50)\n",
        "\n",
        "            al_clean_tensor_wrongs = torch.cat([al_clean_tensor_wrongs, al_clean_wrongs])\n",
        "            ep_clean_tensor_wrongs = torch.cat([ep_clean_tensor_wrongs, ep_clean_wrongs])\n",
        "            sc_clean_tensor_wrongs = torch.cat([sc_clean_tensor_wrongs, sc_clean_wrongs])\n",
        "            ent_clean_tensor_wrongs = torch.cat([ent_clean_tensor_wrongs, ent_clean_wrongs])\n",
        "\n",
        "    print(\"PREPARING CLEAN DATA FOR CORRECTS....\")\n",
        "    al_clean_tensor_corrects, ep_clean_tensor_corrects, sc_clean_tensor_corrects, ent_clean_tensor_corrects = al_clean_tensor_corrects.numpy(), ep_clean_tensor_corrects.numpy(), sc_clean_tensor_corrects.numpy(), ent_clean_tensor_corrects.numpy()\n",
        "    print(\"Done....\")\n",
        "\n",
        "    print(\"PREPARING CLEAN DATA FOR WRONGS...\")\n",
        "    al_clean_tensor_wrongs, ep_clean_tensor_wrongs, sc_clean_tensor_wrongs, ent_clean_tensor_wrongs = al_clean_tensor_wrongs.numpy(), ep_clean_tensor_wrongs.numpy(), sc_clean_tensor_wrongs.numpy(), ent_clean_tensor_wrongs.numpy()\n",
        "    print(\"Done....\")\n",
        "\n",
        "    print(\"\\nMean of the aleatoric uncertainty values for all the errors : \", al_clean_tensor_wrongs.mean())\n",
        "    print(\"Mean of the aleatoric uncertainty values for all the corrects \", al_clean_tensor_corrects.mean())\n",
        "\n",
        "    print(\"\\nMean of the epistemic uncertainty values for all the errors : \", ep_clean_tensor_wrongs.mean())\n",
        "    print(\"Mean of the epistemic uncertainty values for all the corrects \", ep_clean_tensor_corrects.mean())\n",
        "\n",
        "    print(\"\\nMean of the scibilic uncertainty values for all the errors : \", sc_clean_tensor_wrongs.mean())\n",
        "    print(\"Mean of the scibilic uncertainty values for all the corrects \", sc_clean_tensor_corrects.mean())\n",
        "\n",
        "    print(\"\\nMean of the entropy values for all the errors : \", ent_clean_tensor_wrongs.mean())\n",
        "    print(\"Mean of the entropy values for all the corrects \", ent_clean_tensor_corrects.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukuFbQL33DJ"
      },
      "source": [
        "if input_a == 2:\n",
        "\n",
        "    model_attack = LeNet_dropout()\n",
        "    ckpt_attack = torch.load('/content/gdrive/MyDrive/checkpointDigitMNIST/LeNet_dropout10.pth.tar')\n",
        "    model_attack.load_state_dict(ckpt_attack['state_dict'])\n",
        "    model_attack.eval()\n",
        "    model_attack = model_attack.to(device)\n",
        "\n",
        "    last_hidden_idx = -3\n",
        "    output_dim = list(model_attack.children())[last_hidden_idx].out_features\n",
        "    print(output_dim)\n",
        "\n",
        "    ################ CLEAN LAST HIDDEN LAYER OUTPUTS ####################\n",
        "    last_hidden_layer_outputs = np.empty((0, output_dim))\n",
        "    predss = np.empty((0))\n",
        "    labelss = np.empty((0))\n",
        "\n",
        "    for i, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model_attack(image)\n",
        "\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        pred = pred.view_as(label)\n",
        "        predss = np.hstack((predss, pred.detach().cpu().numpy()))\n",
        "        labelss = np.hstack((labelss, label.cpu().numpy()))\n",
        "\n",
        "        last_hidden_layer_outputs = np.vstack(\n",
        "            (last_hidden_layer_outputs, model_attack.last_hidden_layer_output(image).detach().cpu().numpy()))\n",
        "\n",
        "    inds_correct = np.where(predss == labelss)[0]\n",
        "\n",
        "    predss = predss[inds_correct]\n",
        "    labelss = labelss[inds_correct]\n",
        "    last_hidden_layer_outputs = last_hidden_layer_outputs[inds_correct]\n",
        "\n",
        "    ################ ADVERSARIAL LAST HIDDEN LAYER OUTPUTS ####################\n",
        "\n",
        "    adv_last_hidden_layer_outputs = np.empty((0, output_dim))\n",
        "\n",
        "    model_attack.eval()\n",
        "    for i, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "        attack = LinfBasicIterativeAttack()\n",
        "        fmodel = PyTorchModel(model_attack, bounds=(0, 1))\n",
        "        raw_advs, clipped_advs, success = attack(fmodel, image, label, epsilons=[0.20])\n",
        "        pert = torch.tensor(clipped_advs[0])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            adv_last_hidden_layer_outputs = np.vstack(\n",
        "                (adv_last_hidden_layer_outputs, model_attack.last_hidden_layer_output(pert).cpu().numpy()))\n",
        "\n",
        "    adv_last_hidden_layer_outputs = adv_last_hidden_layer_outputs[inds_correct]\n",
        "\n",
        "    ################ NOSIY LAST HIDDEN LAYER OUTPUTS ####################\n",
        "\n",
        "    noisy_last_hidden_layer_outputs = np.empty((0, output_dim))\n",
        "\n",
        "    model_attack.eval()\n",
        "    for i, (image, label) in enumerate(train_loader):\n",
        "        image = image.to(device)\n",
        "        label = label.to(device)\n",
        "        noisy_image = noise(image, 0.20, 0, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            noisy_last_hidden_layer_outputs = np.vstack(\n",
        "                (noisy_last_hidden_layer_outputs, model_attack.last_hidden_layer_output(noisy_image).cpu().numpy()))\n",
        "\n",
        "    noisy_last_hidden_layer_outputs = noisy_last_hidden_layer_outputs[inds_correct]\n",
        "\n",
        "    ###############\n",
        "\n",
        "    dig_outputs = {}\n",
        "    dig_outputs_raw = {}\n",
        "    dig_labels = {}\n",
        "    dig_labels_raw = {}\n",
        "    dig_outputs_adv = {}\n",
        "    dig_outputs_adv_raw = {}\n",
        "    dig_outputs_noisy = {}\n",
        "    dig_outputs_noisy_raw = {}\n",
        "\n",
        "    for i in range(10):\n",
        "        inds_i = np.where(predss == i)[0]\n",
        "\n",
        "        dig_outputs_raw[i] = last_hidden_layer_outputs[inds_i]\n",
        "        dummy = dig_outputs_raw[i].copy()\n",
        "\n",
        "        dig_labels_raw[i] = labelss[inds_i]\n",
        "        dummy_labels = dig_labels_raw[i].copy()\n",
        "\n",
        "        dig_outputs_adv_raw[i] = adv_last_hidden_layer_outputs[inds_i]\n",
        "        dummy_adv = dig_outputs_adv_raw[i].copy()\n",
        "\n",
        "        dig_outputs_noisy_raw[i] = noisy_last_hidden_layer_outputs[inds_i]\n",
        "        dummy_noisy = dig_outputs_noisy_raw[i].copy()\n",
        "\n",
        "        rows = dummy.shape[0]\n",
        "\n",
        "        dig_outputs[i] = dummy\n",
        "        dig_labels[i] = dummy_labels\n",
        "        dig_outputs_adv[i] = dummy_adv\n",
        "        dig_outputs_noisy[i] = dummy_noisy\n",
        "\n",
        "    for i in range(10):\n",
        "        filename = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_outputs_' + str(i) + '.npy'\n",
        "        np.save(filename, dig_outputs[i])\n",
        "\n",
        "        filename2 = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_labels_' + str(i) + '.npy'\n",
        "        np.save(filename2, dig_labels[i])\n",
        "\n",
        "        filename3 = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_outputs_adv_' + str(i) + '.npy'\n",
        "        np.save(filename3, dig_outputs_adv[i])\n",
        "\n",
        "        filename4 = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_outputs_noisy' + str(i) + '.npy'\n",
        "        np.save(filename4, dig_outputs_noisy[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By4iQe8p368V"
      },
      "source": [
        "if input_a == 3:\n",
        "\n",
        "    my_mlp_model = MLP()\n",
        "    my_mlp_model = my_mlp_model.to(device)\n",
        "\n",
        "    model_attack_new = LeNet_dropout()\n",
        "    ckpt_attack_new = torch.load('/content/gdrive/MyDrive/checkpointDigitMNIST/LeNet_dropout10.pth.tar')\n",
        "    model_attack_new.load_state_dict(ckpt_attack_new['state_dict'])\n",
        "    model_attack_new.eval()\n",
        "    model_attack_new = model_attack_new.to(device)\n",
        "\n",
        "\n",
        "    dig_labels = {}\n",
        "    dig_outputs = {}\n",
        "    dig_outputs_adv = {}\n",
        "    dig_outputs_noisy = {}\n",
        "\n",
        "    data = np.empty((0, 128))\n",
        "    labels = np.empty((0, 1))\n",
        "\n",
        "    for i in range(10):\n",
        "        filename = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_outputs_' + str(i) + '.npy'\n",
        "        dig_outputs[i] = np.load(filename)\n",
        "\n",
        "        filename3 = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_outputs_adv_' + str(i) + '.npy'\n",
        "        dig_outputs_adv[i] = np.load(filename3)\n",
        "\n",
        "        filename4 = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_outputs_noisy' + str(i) + '.npy'\n",
        "        dig_outputs_noisy[i] = np.load(filename4)\n",
        "\n",
        "        filename2 = '/content/gdrive/MyDrive/checkpointDigitMNIST/dig_labels_' + str(i) + '.npy'\n",
        "        dig_labels[i] = np.load(filename2)\n",
        "\n",
        "        dig_labels[i] = np.expand_dims(dig_labels[i], axis=1)\n",
        "\n",
        "        data = np.vstack((data, dig_outputs[i]))\n",
        "        data = np.vstack((data, dig_outputs_adv[i]))\n",
        "        data = np.vstack((data, dig_outputs_noisy[i]))\n",
        "        labels = np.vstack((labels, dig_labels[i]))\n",
        "        labels = np.vstack((labels, dig_labels[i]))\n",
        "        labels = np.vstack((labels, dig_labels[i]))\n",
        "\n",
        "    labels = np.squeeze(labels, axis=1)\n",
        "\n",
        "    print(\"labels shape \", labels.shape)\n",
        "    print(\"data shape \", data.shape)\n",
        "\n",
        "    train_data = []\n",
        "    for i in range(len(data)):\n",
        "        train_data.append([data[i], labels[i]])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=128)\n",
        "\n",
        "    my_mlp_model.train()\n",
        "\n",
        "    lr = 0.001\n",
        "    opt = optim.Adam(my_mlp_model.parameters(), lr=lr)\n",
        "    T = 1\n",
        "\n",
        "    my_mlp_model.train()\n",
        "\n",
        "    for epoch in range(50):\n",
        "\n",
        "        total_err = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            data = data.float()\n",
        "            target = target.long()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            output = my_mlp_model(data)\n",
        "            loss = F.nll_loss(F.log_softmax(output, dim=1), target)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_err += (output.max(dim=1)[1] != target).sum().item()\n",
        "            if batch_idx % 100 == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)] lr: {}\\tLoss: {:.6f}'\n",
        "                      .format(epoch, batch_idx * len(data),\n",
        "                              len(train_loader.dataset),\n",
        "                              100. * batch_idx / len(train_loader),\n",
        "                              lr, loss.data))\n",
        "        print('*******Train Epoch: {} error is {}'.format(epoch + 1, total_err))\n",
        "\n",
        "    my_mlp_model.eval()\n",
        "    print('Save /content/gdrive/MyDrive/checkpointDigitMNIST/' + 'MLP_Modell' + '.pth.tar')\n",
        "    state = {'state_dict': my_mlp_model.state_dict()}\n",
        "    filename = '/content/gdrive/MyDrive/checkpointDigitMNIST/' + 'MLP_Modell' + '.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "\n",
        "    my_mlp_model_2 = MLP()\n",
        "    ckpt_dropout = torch.load('/content/gdrive/MyDrive/checkpointDigitMNIST/MLP_Modell.pth.tar')\n",
        "    my_mlp_model_2.load_state_dict(ckpt_dropout['state_dict'])\n",
        "    my_mlp_model_2.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZqLegTH39iY"
      },
      "source": [
        "if input_a == 4:\n",
        "\n",
        "    ckpt_dropout = torch.load('/content/gdrive/MyDrive/checkpointDigitMNIST/LeNet_dropout10.pth.tar')\n",
        "    model_dropout.load_state_dict(ckpt_dropout['state_dict'])\n",
        "    model_dropout.eval()\n",
        "    model_dropout = model_dropout.to(device)\n",
        "\n",
        "    my_mlp_model_2 = MLP()\n",
        "    ckpt_dropout = torch.load('/content/gdrive/MyDrive/checkpointDigitMNIST/MLP_Modell.pth.tar')\n",
        "    my_mlp_model_2.load_state_dict(ckpt_dropout['state_dict'])\n",
        "    my_mlp_model_2.eval()\n",
        "    my_mlp_model_2 = my_mlp_model_2.to(device)\n",
        "\n",
        "\n",
        "    print(\"Chose attack type: \\nDeepfool (1)\\nFGSM  (2)\\nBIM  (3)\\nCW  (4)\\nPGD  (5)\\n\")\n",
        "\n",
        "    input_c = int(input())\n",
        "\n",
        "    print(\"Enter epsilon value : \\n\")\n",
        "\n",
        "    eps = float(input())\n",
        "    eps_str = str(int((eps + 0.0000001) * 100))\n",
        "\n",
        "    if input_c == 1:\n",
        "        attack = LinfDeepFoolAttack()\n",
        "        fmodel = PyTorchModel(model_dropout, bounds=(0, 1))\n",
        "        attack_type = \"Deepfool\"\n",
        "        eps_noisy = eps\n",
        "    elif input_c == 2:\n",
        "        attack = LinfFastGradientAttack()\n",
        "        fmodel = PyTorchModel(model_dropout, bounds=(0, 1))\n",
        "        attack_type = \"FGSM\"\n",
        "        eps_noisy = eps\n",
        "    elif input_c == 3:\n",
        "        attack = LinfBasicIterativeAttack()\n",
        "        fmodel = PyTorchModel(model_dropout, bounds=(0, 1))\n",
        "        attack_type = \"BIM\"\n",
        "        eps_noisy = eps\n",
        "    elif input_c == 4:\n",
        "        #eps 0.3 için L2 eps : 4\n",
        "        #eps 0.12 için  L2 eps : 1.61\n",
        "        if eps == 0.3:\n",
        "            eps = 4\n",
        "            eps_noisy = 0.3\n",
        "        if eps == 0.12:\n",
        "            eps = 1.61\n",
        "            eps_noisy = 0.12\n",
        "        print(eps)\n",
        "        attack = L2CarliniWagnerAttack(steps=1000)\n",
        "        fmodel = PyTorchModel(model_dropout, bounds=(0, 1))\n",
        "        attack_type = \"CW\"\n",
        "    elif input_c == 5:\n",
        "        attack = LinfPGD()\n",
        "        fmodel = PyTorchModel(model_dropout, bounds=(0, 1))\n",
        "        attack_type = \"PGD\"\n",
        "        eps_noisy = eps\n",
        "\n",
        "    print(\"Chosen attack type is \", attack_type)\n",
        "    print(\"Chosen epsilon is \", eps)\n",
        "\n",
        "    print(\"Prepared data will be stored at: \")\n",
        "    print(f\"/content/gdrive/MyDrive/checkpointDigitMNIST/all_numpy_{attack_type}_0{eps_str}.npy\")\n",
        "\n",
        "    print(\"Beginning time is : \")\n",
        "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "    first_time = datetime.datetime.now()\n",
        "\n",
        "    al_clean_tensor = torch.empty(0)\n",
        "    ep_clean_tensor = torch.empty(0)\n",
        "    sc_clean_tensor = torch.empty(0)\n",
        "    ent_clean_tensor = torch.empty(0)\n",
        "    preds_clean_tensor = torch.empty(0)\n",
        "    distances_clean_tensor = torch.empty(0)\n",
        "\n",
        "    al_noisy_tensor = torch.empty(0)\n",
        "    ep_noisy_tensor = torch.empty(0)\n",
        "    sc_noisy_tensor = torch.empty(0)\n",
        "    ent_noisy_tensor = torch.empty(0)\n",
        "    preds_noisy_tensor = torch.empty(0)\n",
        "    distances_noisy_tensor = torch.empty(0)\n",
        "\n",
        "    al_dirty_tensor = torch.empty(0)\n",
        "    ep_dirty_tensor = torch.empty(0)\n",
        "    sc_dirty_tensor = torch.empty(0)\n",
        "    ent_dirty_tensor = torch.empty(0)\n",
        "    preds_dirty_tensor = torch.empty(0)\n",
        "    distances_dirty_tensor = torch.empty(0)\n",
        "\n",
        "    preds_clean_tensor = preds_clean_tensor.to(device)\n",
        "    preds_noisy_tensor = preds_noisy_tensor.to(device)\n",
        "    preds_dirty_tensor = preds_dirty_tensor.to(device)\n",
        "\n",
        "    for test_images, test_labels in test_loader:\n",
        "\n",
        "        test_images = test_images.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "\n",
        "        model_dropout.eval()\n",
        "        sample_image = test_images\n",
        "        sample_label = test_labels\n",
        "        outputs = model_dropout(test_images)\n",
        "        preds = outputs.max(dim=1)[1]\n",
        "        corrects = (preds == test_labels)\n",
        "        sample_image = sample_image[corrects]\n",
        "        sample_label = sample_label[corrects]\n",
        "        preds = preds[corrects]\n",
        "        preds = preds.to(device)\n",
        "\n",
        "        al_clean, ep_clean, sc_clean, ent_clean = predict_uncertainties(model_dropout, sample_image, 50)\n",
        "\n",
        "\n",
        "        al_clean_tensor = torch.cat([al_clean_tensor, al_clean])\n",
        "        ep_clean_tensor = torch.cat([ep_clean_tensor, ep_clean])\n",
        "        sc_clean_tensor = torch.cat([sc_clean_tensor, sc_clean])\n",
        "        ent_clean_tensor = torch.cat([ent_clean_tensor, ent_clean])\n",
        "        preds_clean_tensor = torch.cat([preds_clean_tensor, preds])\n",
        "\n",
        "        model_dropout.eval()\n",
        "        with torch.no_grad():\n",
        "            last_hidden_layer_outputs_clean = model_dropout.last_hidden_layer_output(sample_image).detach().cpu().numpy()\n",
        "\n",
        "        distances_clean = calculate_distance(my_mlp_model_2, preds, last_hidden_layer_outputs_clean)\n",
        "\n",
        "        distances_clean_tensor = torch.cat([distances_clean_tensor, distances_clean])\n",
        "\n",
        "#########################\n",
        "\n",
        "        image_noisy = noise(sample_image, eps_noisy, 0, 1)\n",
        "\n",
        "        al_noisy, ep_noisy, sc_noisy, ent_noisy = predict_uncertainties(model_dropout, image_noisy, 50)\n",
        "\n",
        "        al_noisy_tensor = torch.cat([al_noisy_tensor, al_noisy])\n",
        "        ep_noisy_tensor = torch.cat([ep_noisy_tensor, ep_noisy])\n",
        "        sc_noisy_tensor = torch.cat([sc_noisy_tensor, sc_noisy])\n",
        "        ent_noisy_tensor = torch.cat([ent_noisy_tensor, ent_noisy])\n",
        "\n",
        "        model_dropout.eval()\n",
        "        outputs_noisy = model_dropout(image_noisy)\n",
        "        preds_noisy = outputs_noisy.max(dim=1)[1]\n",
        "\n",
        "        preds_noisy_tensor = torch.cat([preds_noisy_tensor, preds_noisy])\n",
        "\n",
        "\n",
        "        model_dropout.eval()\n",
        "        with torch.no_grad():\n",
        "            last_hidden_layer_outputs_noisy = model_dropout.last_hidden_layer_output(image_noisy).detach().cpu().numpy()\n",
        "\n",
        "        distances_noisy = calculate_distance(my_mlp_model_2, preds_noisy, last_hidden_layer_outputs_noisy)\n",
        "\n",
        "\n",
        "        distances_noisy_tensor = torch.cat([distances_noisy_tensor, distances_noisy])\n",
        "\n",
        "#########################\n",
        "\n",
        "        raw_advs, clipped_advs, success = attack(fmodel, sample_image, sample_label, epsilons=[eps])\n",
        "        image_dirty = torch.tensor(clipped_advs[0])\n",
        "\n",
        "        al_dirty, ep_dirty, sc_dirty, ent_dirty = predict_uncertainties(model_dropout, image_dirty, 50)\n",
        "\n",
        "        al_dirty_tensor = torch.cat([al_dirty_tensor, al_dirty])\n",
        "        ep_dirty_tensor = torch.cat([ep_dirty_tensor, ep_dirty])\n",
        "        sc_dirty_tensor = torch.cat([sc_dirty_tensor, sc_dirty])\n",
        "        ent_dirty_tensor = torch.cat([ent_dirty_tensor, ent_dirty])\n",
        "\n",
        "        model_dropout.eval()\n",
        "        outputs_dirty = model_dropout(image_dirty)\n",
        "        preds_dirty = outputs_dirty.max(dim=1)[1]\n",
        "\n",
        "        preds_dirty_tensor = torch.cat([preds_dirty_tensor, preds_dirty])\n",
        "\n",
        "        model_dropout.eval()\n",
        "        with torch.no_grad():\n",
        "            last_hidden_layer_outputs_dirty = model_dropout.last_hidden_layer_output(image_dirty).detach().cpu().numpy()\n",
        "\n",
        "        distances_dirty = calculate_distance(my_mlp_model_2, preds_dirty, last_hidden_layer_outputs_dirty)\n",
        "\n",
        "        distances_dirty_tensor = torch.cat([distances_dirty_tensor, distances_dirty])\n",
        "\n",
        "    #########################\n",
        "\n",
        "    print(\"PREPARING CLEAN DATA....\")\n",
        "\n",
        "    al_clean_tensor, ep_clean_tensor, sc_clean_tensor, ent_clean_tensor = al_clean_tensor.cpu().numpy(), ep_clean_tensor.cpu().numpy(), sc_clean_tensor.cpu().numpy(), ent_clean_tensor.cpu().numpy()\n",
        "    \n",
        "    zero_clean_tensor = np.zeros_like(al_clean_tensor)\n",
        "\n",
        "    distances_clean_tensor = distances_clean_tensor.cpu().numpy()\n",
        "\n",
        "    preds_clean_tensor = preds_clean_tensor.cpu().numpy()\n",
        "\n",
        "    al_clean_tensor = al_clean_tensor.reshape(al_clean_tensor.shape[0], 1)\n",
        "    ep_clean_tensor = ep_clean_tensor.reshape(ep_clean_tensor.shape[0], 1)\n",
        "    sc_clean_tensor = sc_clean_tensor.reshape(sc_clean_tensor.shape[0], 1)\n",
        "    ent_clean_tensor = ent_clean_tensor.reshape(ent_clean_tensor.shape[0], 1)\n",
        "    zero_clean_tensor = zero_clean_tensor.reshape(zero_clean_tensor.shape[0], 1)\n",
        "    preds_clean_tensor = preds_clean_tensor.reshape(preds_clean_tensor.shape[0], 1)\n",
        "    distances_clean_tensor = distances_clean_tensor.reshape(distances_clean_tensor.shape[0],1)\n",
        "\n",
        "    clean_tensor_all = np.concatenate((al_clean_tensor,ep_clean_tensor,sc_clean_tensor,ent_clean_tensor,preds_clean_tensor,distances_clean_tensor,zero_clean_tensor),axis=1)\n",
        "    print(\"Done....\")\n",
        "\n",
        "    print(\"PREPARING NOISY DATA....\")\n",
        "\n",
        "    al_noisy_tensor, ep_noisy_tensor, sc_noisy_tensor, ent_noisy_tensor = al_noisy_tensor.cpu().numpy(), ep_noisy_tensor.cpu().numpy(), sc_noisy_tensor.cpu().numpy(), ent_noisy_tensor.cpu().numpy()\n",
        "\n",
        "    zero_noisy_tensor = np.zeros_like(al_noisy_tensor)\n",
        "\n",
        "    distances_noisy_tensor = distances_noisy_tensor.cpu().numpy()\n",
        "\n",
        "    preds_noisy_tensor = preds_noisy_tensor.cpu().numpy()\n",
        "\n",
        "    al_noisy_tensor = al_noisy_tensor.reshape(al_noisy_tensor.shape[0], 1)\n",
        "    ep_noisy_tensor = ep_noisy_tensor.reshape(ep_noisy_tensor.shape[0], 1)\n",
        "    sc_noisy_tensor = sc_noisy_tensor.reshape(sc_noisy_tensor.shape[0], 1)\n",
        "    ent_noisy_tensor = ent_noisy_tensor.reshape(ent_noisy_tensor.shape[0], 1)\n",
        "    zero_noisy_tensor = zero_noisy_tensor.reshape(zero_noisy_tensor.shape[0], 1)\n",
        "    preds_noisy_tensor = preds_noisy_tensor.reshape(preds_noisy_tensor.shape[0], 1)\n",
        "    distances_noisy_tensor = distances_noisy_tensor.reshape(distances_noisy_tensor.shape[0],1)\n",
        "\n",
        "\n",
        "    noisy_tensor_all = np.concatenate((al_noisy_tensor,ep_noisy_tensor,sc_noisy_tensor,ent_noisy_tensor,preds_noisy_tensor,distances_noisy_tensor,zero_noisy_tensor),axis=1)\n",
        "    print(\"Done....\")\n",
        "\n",
        "    print(\"PREPARING DIRTY DATA....\")\n",
        "\n",
        "    al_dirty_tensor, ep_dirty_tensor, sc_dirty_tensor, ent_dirty_tensor = al_dirty_tensor.cpu().numpy(), ep_dirty_tensor.cpu().numpy(), sc_dirty_tensor.cpu().numpy(), ent_dirty_tensor.cpu().numpy()\n",
        "\n",
        "    ones_dirty_tensor = np.ones_like(ep_dirty_tensor)\n",
        "\n",
        "    distances_dirty_tensor = distances_dirty_tensor.cpu().numpy()\n",
        "\n",
        "    preds_dirty_tensor = preds_dirty_tensor.cpu().numpy()\n",
        "\n",
        "    al_dirty_tensor = al_dirty_tensor.reshape(al_dirty_tensor.shape[0], 1)\n",
        "    ep_dirty_tensor = ep_dirty_tensor.reshape(ep_dirty_tensor.shape[0], 1)\n",
        "    sc_dirty_tensor = sc_dirty_tensor.reshape(sc_dirty_tensor.shape[0], 1)\n",
        "    ent_dirty_tensor = ent_dirty_tensor.reshape(ent_dirty_tensor.shape[0], 1)\n",
        "    ones_dirty_tensor = ones_dirty_tensor.reshape(ones_dirty_tensor.shape[0], 1)\n",
        "    preds_dirty_tensor = preds_dirty_tensor.reshape(preds_dirty_tensor.shape[0], 1)\n",
        "    distances_dirty_tensor = distances_dirty_tensor.reshape(distances_dirty_tensor.shape[0],1)\n",
        "\n",
        "\n",
        "    dirty_tensor_all = np.concatenate((al_dirty_tensor,ep_dirty_tensor,sc_dirty_tensor,ent_dirty_tensor,preds_dirty_tensor,distances_dirty_tensor,ones_dirty_tensor),axis=1)\n",
        "\n",
        "    print(\"Done....\")\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(\"epistemic uncertainty clean shape \", ep_clean_tensor.shape)\n",
        "    print(\"distance tensor clean shape \", distances_clean_tensor.shape)\n",
        "    print(\"distance tensor noisy shape \", distances_noisy_tensor.shape)\n",
        "    print(\"distance tensor dirty shape \", distances_dirty_tensor.shape)\n",
        "    print(\"preds tensor dirty shape \", preds_dirty_tensor.shape)\n",
        "    print(\"clean data all shape \", clean_tensor_all.shape)\n",
        "    print(\"noisy data all shape \", noisy_tensor_all.shape)\n",
        "    print(\"dirty data all shape \", dirty_tensor_all.shape)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    all_tensor_all = np.concatenate((clean_tensor_all,noisy_tensor_all,dirty_tensor_all),axis=0)\n",
        "\n",
        "    print(\"shape of all data is: \")\n",
        "    print(all_tensor_all.shape)\n",
        "    print()\n",
        "\n",
        "    np.save(f\"/content/gdrive/MyDrive/checkpointDigitMNIST/all_numpy_{attack_type}_0{eps_str}.npy\", all_tensor_all)\n",
        "\n",
        "    print(\"For Clean data\")\n",
        "    print(\"Number of features : \", al_clean_tensor.shape[0])\n",
        "\n",
        "    print(\"aleatoric :\", al_clean_tensor.mean())\n",
        "    print(\"epistemic :\", ep_clean_tensor.mean())\n",
        "    print(\"scibilic :\", sc_clean_tensor.mean())\n",
        "    print(\"entropy :\", ent_clean_tensor.mean())\n",
        "    print(\"distance :\", distances_clean_tensor.mean())\n",
        "\n",
        "    print(\"For Noisy data\")\n",
        "    print(\"Number of features : \", al_noisy_tensor.shape[0])\n",
        "\n",
        "\n",
        "    print(\"aleatoric :\", al_noisy_tensor.mean())\n",
        "    print(\"epistemic :\", ep_noisy_tensor.mean())\n",
        "    print(\"scibilic :\", sc_noisy_tensor.mean())\n",
        "    print(\"entropy :\", ent_noisy_tensor.mean())\n",
        "    print(\"distance :\", distances_noisy_tensor.mean())\n",
        "\n",
        "    print(\"For Dirty data\")\n",
        "    print(\"Number of features : \", al_dirty_tensor.shape[0])\n",
        "\n",
        "\n",
        "    print(\"aleatoric :\", al_dirty_tensor.mean())\n",
        "    print(\"epistemic :\", ep_dirty_tensor.mean())\n",
        "    print(\"scibilic :\", sc_dirty_tensor.mean())\n",
        "    print(\"entropy :\", ent_dirty_tensor.mean())\n",
        "    print(\"distance :\", distances_dirty_tensor.mean())\n",
        "    print()\n",
        "\n",
        "    print(\"End time is : \")\n",
        "    print(strftime(\"%Y-%m-%d %H:%M:%S\", gmtime()))\n",
        "    print()\n",
        "\n",
        "    later_time = datetime.datetime.now()\n",
        "    difference = later_time - first_time\n",
        "    seconds_in_day = 24 * 60 * 60\n",
        "    print(\"Duration in minutes and seconds is : \")\n",
        "    print(divmod(difference.days * seconds_in_day + difference.seconds, 60))\n",
        "\n",
        "    print(\"####################################\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5t1eQOv3_14"
      },
      "source": [
        "if input_a == 5:\n",
        "\n",
        "    print(\"Chose data attack type: \\nDeepfool (1)\\nFGSM  (2)\\nBIM  (3)\\nCW  (4)\\nPGD  (5)\\n\")\n",
        "\n",
        "    input_c = int(input())\n",
        "\n",
        "    if input_c == 1:\n",
        "        attack_type = \"Deepfool\"\n",
        "    elif input_c == 2:\n",
        "        attack_type = \"FGSM\"\n",
        "    elif input_c == 3:\n",
        "        attack_type = \"BIM\"\n",
        "    elif input_c == 4:\n",
        "        attack_type = \"CW\"\n",
        "    elif input_c == 5:\n",
        "        attack_type = \"PGD\"\n",
        "\n",
        "    directory = '/content/gdrive/MyDrive/checkpointDigitMNIST/' + attack_type\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "    print(\"Enter epsilon value : \\n\")\n",
        "\n",
        "    eps = float(input())\n",
        "    eps_str = str(int((eps+0.0000001)*100))\n",
        "\n",
        "    print(\"Chosen attack type is \", attack_type)\n",
        "    print(\"Chosen epsilon is \", eps)\n",
        "\n",
        "    train_data = f\"/content/gdrive/MyDrive/checkpointDigitMNIST/all_numpy_{attack_type}_0{eps_str}.npy\"\n",
        "    tail = f\"_{attack_type}_0{eps_str}.npy\"\n",
        "\n",
        "\n",
        "    train_data_all_numpy = np.load(train_data)\n",
        "    train_data_all_numpy_shuffled = shuffle(train_data_all_numpy, random_state=0)\n",
        "\n",
        "    cols = [[0], [1], [2], [3], [5], [0, 1, 2, 3, 5]]\n",
        "\n",
        "    fpr_list = [\"fpr_aleatoric\", \"fpr_epistemic\", \"fpr_scibilic\", \"fpr_entropy\", \"fpr_distance\", \"fpr_All\"]\n",
        "\n",
        "    tpr_list = [\"tpr_aleatoric\", \"tpr_epistemic\", \"tpr_scibilic\", \"tpr_entropy\", \"fpr_distance\", \"tpr_All\"]\n",
        "\n",
        "    roc_auc_list = [\"roc_auc_aleatoric\", \"roc_auc_epistemic\", \"roc_auc_scibilic\", \"roc_auc_entropy\", \"roc_auc_distance\", \"roc_auc_All\"]\n",
        "\n",
        "    legend_label = ['Aleatoric', 'Epistemic', 'Scibilic', 'Entropy', 'Distance', 'All']\n",
        "\n",
        "    for i,c in enumerate(cols):\n",
        "\n",
        "        col_idx = np.array(c)\n",
        "\n",
        "        X_train = train_data_all_numpy_shuffled[:, col_idx]\n",
        "        y_train = train_data_all_numpy_shuffled[:, -1]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        scaler.fit(X_train)\n",
        "        X_train = scaler.transform(X_train)\n",
        "\n",
        "        logisticRegr = LogisticRegressionCV()\n",
        "        lr = logisticRegr.fit(X_train, y_train)\n",
        "\n",
        "        probs = lr.predict_proba(X_train)\n",
        "        preds = probs[:, 1]\n",
        "\n",
        "        fpr, tpr, threshold = metrics.roc_curve(y_train, preds)\n",
        "\n",
        "        roc_auc = []\n",
        "        roc_auc.append(metrics.auc(fpr, tpr))\n",
        "\n",
        "        fpr = np.array(fpr)\n",
        "        np.save(\"/content/gdrive/MyDrive/checkpointDigitMNIST/\" + attack_type + \"/\" + fpr_list[i] + tail, fpr)\n",
        "\n",
        "        tpr = np.array(tpr)\n",
        "        np.save(\"/content/gdrive/MyDrive/checkpointDigitMNIST/\" + attack_type + \"/\" + tpr_list[i] + tail, tpr)\n",
        "\n",
        "        roc_auc = np.array(roc_auc)\n",
        "        np.save(\"/content/gdrive/MyDrive/checkpointDigitMNIST/\" + attack_type + \"/\" + roc_auc_list[i] + tail, roc_auc)\n",
        "\n",
        "    cols = [[0], [1], [2], [3], [5], [0, 1, 2, 3, 5]]\n",
        "\n",
        "    fpr_list = [\"fpr_aleatoric\", \"fpr_epistemic\", \"fpr_scibilic\", \"fpr_entropy\", \"fpr_distance\", \"fpr_All\"]\n",
        "\n",
        "    tpr_list = [\"tpr_aleatoric\", \"tpr_epistemic\", \"tpr_scibilic\", \"tpr_entropy\", \"fpr_distance\", \"tpr_All\"]\n",
        "\n",
        "    roc_auc_list = [\"roc_auc_aleatoric\", \"roc_auc_epistemic\", \"roc_auc_scibilic\", \"roc_auc_entropy\", \"roc_auc_distance\", \"roc_auc_All\"]\n",
        "\n",
        "    legend_label = ['Aleatoric', 'Epistemic', 'Scibilic', 'Entropy', 'Distance', 'All']\n",
        "\n",
        "    for i,c in enumerate(cols):\n",
        "        lw = 2\n",
        "        fpr = np.load(\"/content/gdrive/MyDrive/checkpointDigitMNIST/\" + attack_type + \"/\" + fpr_list[i] + tail)\n",
        "        tpr = np.load(\"/content/gdrive/MyDrive/checkpointDigitMNIST/\" + attack_type + \"/\" + tpr_list[i] + tail)\n",
        "        roc_auc = np.load(\"/content/gdrive/MyDrive/checkpointDigitMNIST/\" + attack_type + \"/\" + roc_auc_list[i] + tail)[0]\n",
        "        print(roc_auc)\n",
        "\n",
        "        plt.plot(fpr, tpr,lw=lw, label='%s (area = %0.2f)' % (legend_label[i], roc_auc))\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('FPR', fontsize=14)\n",
        "        plt.ylabel('TPR', fontsize=14)\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.title(f'Under {attack_type} attack with epsilon = {str(eps)}')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
